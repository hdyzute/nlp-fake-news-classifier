{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N2e9nxpvHLkF",
        "GWBkBALA3Qif",
        "mFKXN3Rq3TiD",
        "8ELJhIlZ3YGZ",
        "_HtDsZkBGpAo",
        "qh988dA9XLwm",
        "uCfviJ9RGrqY",
        "aMOMNs5zVh8D",
        "AHgBKeU-GvR9",
        "ohHSKmOeG9Fg"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdyzute/nlp-fake-news-classifier/blob/main/nlp_fake_news_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Project**\n",
        "\n",
        "##**Problem stament :**     \n",
        "\n",
        "The widespread dissemination of fake news and propaganda presents serious societal risks, including the erosion of public trust, political polarization, manipulation of elections, and the spread of harmful misinformation during crises such as pandemics or conflicts. From an NLP perspective, detecting fake news is fraught with challenges. Linguistically, fake news often mimics the tone and structure of legitimate journalism, making it difficult to distinguish using surface-level features. The absence of reliable and up-to-date labeled datasets, especially across multiple languages and regions, hampers the effectiveness of supervised learning models. Additionally, the dynamic and adversarial nature of misinformation means that malicious actors constantly evolve their language and strategies to bypass detection systems. Cultural context, sarcasm, satire, and implicit bias further complicate automated analysis. Moreover, NLP models risk amplifying biases present in training data, leading to unfair classifications and potential censorship of legitimate content. These challenges underscore the need for cautious, context-aware approaches, as the failure to address them can inadvertently contribute to misinformation, rather than mitigate it.\n",
        "\n",
        "\n",
        "\n",
        "Use datasets in link : https://drive.google.com/drive/folders/1mrX3vPKhEzxG96OCPpCeh9F8m_QKCM4z?usp=sharing\n",
        "to complete requirement.\n",
        "\n",
        "## **About dataset:**\n",
        "\n",
        "* **True Articles**:\n",
        "\n",
        "  * **File**: `MisinfoSuperset_TRUE.csv`\n",
        "  * **Sources**:\n",
        "\n",
        "    * Reputable media outlets like **Reuters**, **The New York Times**, **The Washington Post**, etc.\n",
        "\n",
        "* **Fake/Misinformation/Propaganda Articles**:\n",
        "\n",
        "  * **File**: `MisinfoSuperset_FAKE.csv`\n",
        "  * **Sources**:\n",
        "\n",
        "    * **American right-wing extremist websites** (e.g., Redflag Newsdesk, Breitbart, Truth Broadcast Network)\n",
        "    * **Public dataset** from:\n",
        "\n",
        "      * Ahmed, H., Traore, I., & Saad, S. (2017): \"Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques\" *(Springer LNCS 10618)*\n",
        "\n",
        "\n",
        "\n",
        "## **Requirement**\n",
        "\n",
        "A team consisting of three members must complete a project that involves applying the methods learned from the beginning of the course up to the present. The team is expected to follow and document the entire machine learning workflow, which includes the following steps:\n",
        "\n",
        "1. **Data Preprocessing**: Clean and prepare the dataset,etc.\n",
        "\n",
        "2. **Exploratory Data Analysis (EDA)**: Explore and visualize the data.\n",
        "\n",
        "3. **Model Building**: Select and build one or more machine learning models suitable for the problem at hand.\n",
        "\n",
        "4. **Hyperparameter set up**: Set and adjust the model's hyperparameters using appropriate methods to improve performance.\n",
        "\n",
        "5. **Model Training**: Train the model(s) on the training dataset.\n",
        "\n",
        "6. **Performance Evaluation**: Evaluate the trained model(s) using appropriate metrics (e.g., accuracy, precision, recall, F1-score, confusion matrix, etc.) and validate their performance on unseen data.\n",
        "\n",
        "7. **Conclusion**: Summarize the results, discuss the model's strengths and weaknesses, and suggest possible improvements or future work.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eFCzIBshcHmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library"
      ],
      "metadata": {
        "id": "N2e9nxpvHLkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "hRGT2CuDHNBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "LW_-saQCIMiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout,\n",
        "    Input, Concatenate, GlobalMaxPooling1D, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ntbKRFwUWNWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "ghLIYX7MGVDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "GWBkBALA3Qif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ElAxIkho3TSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_data = pd.read_csv(\"/content/drive/MyDrive/DataSet_Misinfo_TRUE.csv\")\n",
        "fake_data = pd.read_csv(\"/content/drive/MyDrive/DataSet_Misinfo_FAKE.csv\")"
      ],
      "metadata": {
        "id": "Z1VIpPJYHA1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "mFKXN3Rq3TiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_data['label'] = 1\n",
        "fake_data['label'] = 0"
      ],
      "metadata": {
        "id": "tsOjMojo3Xha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([true_data, fake_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "Olp9QxX-IkvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "AJXtDJWiI809"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['text'].notna() & (data['text'].str.strip() != '')]"
      ],
      "metadata": {
        "id": "heHjCRnRKqvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Làm sạch văn bản\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "cleaned_texts = []\n",
        "\n",
        "for text in data['text']:\n",
        "    if not isinstance(text, str):\n",
        "        cleaned_texts.append('')\n",
        "        continue\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    cleaned_texts.append(' '.join(tokens))\n",
        "\n",
        "data['cleaned_text'] = cleaned_texts\n",
        "data = data[data['cleaned_text'] != '']"
      ],
      "metadata": {
        "id": "UhaJtz4aJEgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Kiểm tra mất cân bằng lớp\n",
        "class_counts = data['label'].value_counts()\n",
        "print(\"Phân bố lớp:\")\n",
        "print(class_counts)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='label', data=data)\n",
        "plt.title('Phân bố lớp bài báo (1: Thật, 0: Giả)')\n",
        "plt.xlabel('Nhãn')\n",
        "plt.ylabel('Số lượng')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sY0MqWbfL9rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Vector hóa với TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(data['cleaned_text'])\n",
        "y = data['label'].values"
      ],
      "metadata": {
        "id": "XzY-tkthNfAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Chia dữ liệu (cho TF-IDF)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.1, random_state=42, stratify=y\n",
        ")\n",
        "val_size_adjusted = 0.1 / (0.8 + 0.1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# In kích thước các tập\n",
        "print(\"Kích thước tập huấn luyện:\", X_train.shape)\n",
        "print(\"Kích thước tập kiểm định:\", X_val.shape)\n",
        "print(\"Kích thước tập kiểm tra:\", X_test.shape)"
      ],
      "metadata": {
        "id": "jputCmnuOAaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Xử lý imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "8vjVlU4BXW_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Token hóa cho BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_length = 128\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in data['cleaned_text']:\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='np'\n",
        "    )\n",
        "    input_ids.append(encoding['input_ids'][0])\n",
        "    attention_masks.append(encoding['attention_mask'][0])\n",
        "\n",
        "input_ids = np.array(input_ids)\n",
        "attention_masks = np.array(attention_masks)\n",
        "y_bert = data['label'].values"
      ],
      "metadata": {
        "id": "oyAxoLfNVdGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "8ELJhIlZ3YGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "MylOCbVO7Ccl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()"
      ],
      "metadata": {
        "id": "UwOjTlNq7Pjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== LABEL DISTRIBUTION ===\")\n",
        "label_counts = df['label'].value_counts()\n",
        "print(label_counts)\n",
        "print(f\"Label balance ratio: {label_counts.min()/label_counts.max():.2f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "data['label'].value_counts().plot(kind='bar')\n",
        "plt.title('Label Distribution')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qr9fpLF21j1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== TEXT LENGTH ANALYSIS ===\")\n",
        "df['text_length'] = df['text'].str.len()\n",
        "df['cleaned_text_length'] = df['cleaned_text'].str.len()\n",
        "df['word_count'] = df['text'].str.split().str.len()\n",
        "df['cleaned_word_count'] = df['cleaned_text'].str.split().str.len()\n",
        "\n",
        "print(f\"Original text length - Mean: {df['text_length'].mean():.2f}, Std: {df['text_length'].std():.2f}\")\n",
        "print(f\"Cleaned text length - Mean: {df['cleaned_text_length'].mean():.2f}, Std: {df['cleaned_text_length'].std():.2f}\")\n",
        "print(f\"Word count - Mean: {df['word_count'].mean():.2f}, Std: {df['word_count'].std():.2f}\")\n",
        "\n",
        "# Plot text length distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Text length histogram\n",
        "axes[0,0].hist(df['text_length'], bins=50, alpha=0.7, color='blue')\n",
        "axes[0,0].set_title('Original Text Length Distribution')\n",
        "axes[0,0].set_xlabel('Character Count')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "\n",
        "# Cleaned text length histogram\n",
        "axes[0,1].hist(df['cleaned_text_length'], bins=50, alpha=0.7, color='green')\n",
        "axes[0,1].set_title('Cleaned Text Length Distribution')\n",
        "axes[0,1].set_xlabel('Character Count')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "\n",
        "# Word count histogram\n",
        "axes[1,0].hist(df['word_count'], bins=50, alpha=0.7, color='red')\n",
        "axes[1,0].set_title('Word Count Distribution')\n",
        "axes[1,0].set_xlabel('Word Count')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "\n",
        "# Text length by label\n",
        "df.boxplot(column='text_length', by='label', ax=axes[1,1])\n",
        "axes[1,1].set_title('Text Length by Label')\n",
        "axes[1,1].set_xlabel('Label')\n",
        "axes[1,1].set_ylabel('Character Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pE8Xwtuy2Gga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. VOCABULARY\n",
        "from collections import Counter\n",
        "print(\"\\n=== VOCABULARY ANALYSIS ===\")\n",
        "\n",
        "# Function to get word frequencies\n",
        "def get_word_freq(text_series, n_top=20):\n",
        "    all_words = ' '.join(text_series.astype(str)).lower().split()\n",
        "    # Remove basic punctuation\n",
        "    all_words = [re.sub(r'[^a-zA-Z]', '', word) for word in all_words]\n",
        "    all_words = [word for word in all_words if len(word) > 0]\n",
        "    return Counter(all_words).most_common(n_top)\n",
        "\n",
        "# Overall word frequencies\n",
        "top_words_original = get_word_freq(df['text'])\n",
        "top_words_cleaned = get_word_freq(df['cleaned_text'])\n",
        "\n",
        "print(\"Top 20 words in original text:\")\n",
        "for word, freq in top_words_original:\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"\\nTop 20 words in cleaned text:\")\n",
        "for word, freq in top_words_cleaned:\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "# Plot top words\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Original text top words\n",
        "words_orig, freqs_orig = zip(*top_words_original)\n",
        "axes[0].barh(words_orig, freqs_orig)\n",
        "axes[0].set_title('Top 20 Words in Original Text')\n",
        "axes[0].set_xlabel('Frequency')\n",
        "\n",
        "# Cleaned text top words\n",
        "words_clean, freqs_clean = zip(*top_words_cleaned)\n",
        "axes[1].barh(words_clean, freqs_clean)\n",
        "axes[1].set_title('Top 20 Words in Cleaned Text')\n",
        "axes[1].set_xlabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xTJLEmoo2vy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. WORD FREQUENCY BY LABEL\n",
        "print(\"\\n=== WORD FREQUENCY BY LABEL ===\")\n",
        "for label in df['label'].unique():\n",
        "    print(f\"\\nTop words for label {label}:\")\n",
        "    label_text = df[df['label'] == label]['cleaned_text']\n",
        "    top_words_label = get_word_freq(label_text, 15)\n",
        "    for word, freq in top_words_label:\n",
        "        print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "id": "TbKrfEu84y36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. TEXT COMPARISON (Original vs Cleaned)\n",
        "print(\"\\n=== TEXT COMPARISON ===\")\n",
        "df['compression_ratio'] = df['cleaned_text_length'] / df['text_length']\n",
        "print(f\"Average compression ratio: {df['compression_ratio'].mean():.2f}\")\n",
        "print(f\"Text reduction: {(1 - df['compression_ratio'].mean()) * 100:.1f}%\")"
      ],
      "metadata": {
        "id": "ztKBXCQO6WBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
        "summary_stats = {\n",
        "    'Total Samples': len(df),\n",
        "    'Unique Texts': df['text'].nunique(),\n",
        "    'Average Text Length': df['text_length'].mean(),\n",
        "    'Average Word Count': df['word_count'].mean(),\n",
        "    'Labels': df['label'].nunique(),\n",
        "    'Class Balance': df['label'].value_counts().min() / df['label'].value_counts().max()\n",
        "}\n",
        "\n",
        "for key, value in summary_stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "4XhvsdFR6cLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building"
      ],
      "metadata": {
        "id": "_HtDsZkBGpAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "YIepEsMEPvpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train_resampled, y_train_resampled)\n",
        "Accuracy = logreg.score(X_test, y_test)\n",
        "print(Accuracy*100)"
      ],
      "metadata": {
        "id": "3mwVMnOFGrLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = \"The French government announced a national day of mourning after the attack, and President Emmanuel Macron expressed solidarity with the victims during a press conference in Paris.\"\n",
        "encoding = tokenizer(\n",
        "    X,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors='np'\n",
        ")\n",
        "\n",
        "\n",
        "X_transformed = vectorizer.transform([X])\n",
        "\n",
        "prediction_proba_logreg = logreg.predict_proba(X_transformed)\n",
        "\n",
        "probability_true_logreg = prediction_proba_logreg[0][1]\n",
        "\n",
        "if probability_true_logreg >= 0.5:\n",
        "    print(\"This news is True\")\n",
        "else:\n",
        "    print(\"This news is False\")\n"
      ],
      "metadata": {
        "id": "sxdEktdEOqLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = \"Aliens have landed in Germany and established a new world order, claims anonymous source from social media.\"\n",
        "encoding = tokenizer(\n",
        "    X,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors='np'\n",
        ")\n",
        "\n",
        "\n",
        "X_transformed = vectorizer.transform([X])\n",
        "\n",
        "prediction_proba_logreg = logreg.predict_proba(X_transformed)\n",
        "\n",
        "probability_true_logreg = prediction_proba_logreg[0][1]\n",
        "\n",
        "if probability_true_logreg >= 0.5:\n",
        "    print(\"This news is True\")\n",
        "else:\n",
        "    print(\"This news is False\")"
      ],
      "metadata": {
        "id": "1hXx8gj17ysL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive-Bayes"
      ],
      "metadata": {
        "id": "A8jx4KTEPy1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NB = MultinomialNB()\n",
        "NB.fit(X_train_resampled, y_train_resampled)\n",
        "Accuracy = NB.score(X_test, y_test)\n",
        "print(Accuracy*100)"
      ],
      "metadata": {
        "id": "T_uqnnMgOT1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = \"The French government announced a national day of mourning after the attack, and President Emmanuel Macron expressed solidarity with the victims during a press conference in Paris.\"\n",
        "encoding = tokenizer(\n",
        "    X,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors='np'\n",
        ")\n",
        "\n",
        "\n",
        "X_transformed = vectorizer.transform([X])\n",
        "\n",
        "prediction_proba_NB = NB.predict_proba(X_transformed)\n",
        "\n",
        "probability_true_NB = prediction_proba_NB[0][1]\n",
        "\n",
        "if probability_true_NB >= 0.5:\n",
        "    print(\"This news is True\")\n",
        "else:\n",
        "    print(\"This news is False\")\n"
      ],
      "metadata": {
        "id": "U_6hDOKzvabo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = \"Aliens have landed in Germany and established a new world order, claims anonymous source from social media.\"\n",
        "encoding = tokenizer(\n",
        "    X,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors='np'\n",
        ")\n",
        "\n",
        "\n",
        "X_transformed = vectorizer.transform([X])\n",
        "\n",
        "prediction_proba_NB = NB.predict_proba(X_transformed)\n",
        "\n",
        "probability_true_NB = prediction_proba_NB[0][1]\n",
        "\n",
        "if probability_true_NB >= 0.5:\n",
        "    print(\"This news is True\")\n",
        "else:\n",
        "    print(\"This news is False\")\n"
      ],
      "metadata": {
        "id": "QNFFQdKb7-BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN + LSTM (Keras)"
      ],
      "metadata": {
        "id": "qh988dA9XLwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter set up"
      ],
      "metadata": {
        "id": "uCfviJ9RGrqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình tham số\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "c-y5nItCGt2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "aMOMNs5zVh8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization cho CNN + LSTM\n",
        "print(\"Đang thực hiện tokenization...\")\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data['cleaned_text'])"
      ],
      "metadata": {
        "id": "AUfqo0QyWU_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuyển đổi text thành sequences\n",
        "sequences = tokenizer.texts_to_sequences(data['cleaned_text'])\n",
        "X_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "y_sequences = data['label'].values\n",
        "\n",
        "print(f\"Vocab size: {len(tokenizer.word_index)}\")\n",
        "print(f\"Shape của X_sequences: {X_sequences.shape}\")"
      ],
      "metadata": {
        "id": "oYcwDUBVWWoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chia dữ liệu cho CNN + LSTM\n",
        "X_temp_seq, X_test_seq, y_temp_seq, y_test_seq = train_test_split(\n",
        "    X_sequences, y_sequences, test_size=0.1, random_state=42, stratify=y_sequences\n",
        ")\n",
        "val_size_adjusted = 0.1 / (0.8 + 0.1)\n",
        "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_temp_seq, y_temp_seq, test_size=val_size_adjusted, random_state=42, stratify=y_temp_seq\n",
        ")\n",
        "\n",
        "print(\"Kích thước tập dữ liệu cho CNN + LSTM:\")\n",
        "print(f\"Train: {X_train_seq.shape}, Val: {X_val_seq.shape}, Test: {X_test_seq.shape}\")"
      ],
      "metadata": {
        "id": "rTO4ThKHWcIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xử lý class imbalance với class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train_seq),\n",
        "    y=y_train_seq\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "print(f\"Class weights: {class_weight_dict}\")"
      ],
      "metadata": {
        "id": "1QCcDBtTWhEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training"
      ],
      "metadata": {
        "id": "AHgBKeU-GvR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Định nghĩa mô hình CNN + LSTM\n",
        "def create_cnn_lstm_model(vocab_size, embedding_dim, max_length):\n",
        "    \"\"\"\n",
        "    Tạo mô hình CNN + LSTM cho phân loại fake news\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Embedding layer\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, name='embedding'))\n",
        "\n",
        "    # CNN layers\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', name='conv1d_1'))\n",
        "    model.add(MaxPooling1D(pool_size=2, name='maxpool1d_1'))\n",
        "    model.add(Dropout(0.3, name='dropout_1'))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', name='conv1d_2'))\n",
        "    model.add(MaxPooling1D(pool_size=2, name='maxpool1d_2'))\n",
        "    model.add(Dropout(0.3, name='dropout_2'))\n",
        "\n",
        "    # LSTM layers\n",
        "    model.add(LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3, name='lstm_1'))\n",
        "    model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.3, name='lstm_2'))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(64, activation='relu', name='dense_1'))\n",
        "    model.add(BatchNormalization(name='batch_norm'))\n",
        "    model.add(Dropout(0.5, name='dropout_3'))\n",
        "    model.add(Dense(32, activation='relu', name='dense_2'))\n",
        "    model.add(Dropout(0.3, name='dropout_4'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Tạo và compile model\n",
        "print(\"Đang tạo mô hình CNN + LSTM...\")\n",
        "vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
        "print(f\"Vocab size được sử dụng: {vocab_size}\")"
      ],
      "metadata": {
        "id": "l_yeDW_eWjFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_cnn_lstm_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    max_length=MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "# Build model bằng cách truyền input shape\n",
        "model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))"
      ],
      "metadata": {
        "id": "eqac2oejZEVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")"
      ],
      "metadata": {
        "id": "tWQDk8G1Wk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In thông tin mô hình\n",
        "print(\"\\nKiến trúc mô hình:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LFviP2FfWkz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "ZSfpAJtgWph9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "print(\"\\nBắt đầu training...\")\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "EG-CFdtsWpxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đánh giá mô hình\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ĐÁNH GIÁ MÔ HÌNH CNN + LSTM\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "zY_5C7GFWrhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dự đoán trên tập test\n",
        "test_predictions = model.predict(X_test_seq)\n",
        "test_pred_binary = (test_predictions > 0.5).astype(int).flatten()"
      ],
      "metadata": {
        "id": "dRcUyBlnWvYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Performance Evaluation"
      ],
      "metadata": {
        "id": "ohHSKmOeG9Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tính toán metrics\n",
        "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
        "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {f1_score:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_seq, test_pred_binary, target_names=['Fake', 'Real']))"
      ],
      "metadata": {
        "id": "wqO8csiIWz_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_seq, test_pred_binary)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.title('Confusion Matrix - CNN + LSTM')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SBg7sDSHW1_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vẽ biểu đồ training history\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Vẽ biểu đồ quá trình training\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    axes[0, 0].set_title('Model Accuracy')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss')\n",
        "    axes[0, 1].set_title('Model Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Precision\n",
        "    axes[1, 0].plot(history.history['precision'], label='Train Precision')\n",
        "    axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n",
        "    axes[1, 0].set_title('Model Precision')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Precision')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # Recall\n",
        "    axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
        "    axes[1, 1].set_title('Model Recall')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Recall')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "zTkiCIRKW5xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_news(text, model, tokenizer, max_length=MAX_SEQUENCE_LENGTH):\n",
        "    \"\"\"\n",
        "    Dự đoán một bài báo là thật hay giả\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    # Tokenize và padding\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    # Dự đoán\n",
        "    prediction = model.predict(padded_sequence)[0][0]\n",
        "\n",
        "    return {\n",
        "        'prediction': prediction,\n",
        "        'label': 'Real' if prediction > 0.5 else 'Fake',\n",
        "        'confidence': prediction if prediction > 0.5 else 1 - prediction\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZIINSiGDW8Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Breaking news: Scientists discover new planet in our solar system\"\n",
        "result = predict_news(sample_text, model, tokenizer)\n",
        "print(f\"\\nVí dụ dự đoán:\")\n",
        "print(f\"Text: {sample_text}\")\n",
        "print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.4f})\")\n"
      ],
      "metadata": {
        "id": "erk00An9W8j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lưu model\n",
        "model.save('cnn_lstm_fake_news_model.h5')\n",
        "print(\"\\nMô hình đã được lưu vào 'cnn_lstm_fake_news_model.h5'\")"
      ],
      "metadata": {
        "id": "lg157b5nW_1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"Tokenizer đã được lưu vào 'tokenizer.pkl'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ],
      "metadata": {
        "id": "hjLzcWZZXD0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT"
      ],
      "metadata": {
        "id": "JfLTLpASLFwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thiết lập device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "3AI1OVXKLI22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tạo Dataset class cho BERT\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "rZrh8pFGLPdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Chia dữ liệu cho BERT\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Chia dữ liệu với tỷ lệ 80:10:10\n",
        "X_temp_ids, X_test_ids, X_temp_masks, X_test_masks, y_temp_bert, y_test_bert = train_test_split(\n",
        "    input_ids, attention_masks, y_bert, test_size=0.1, random_state=42, stratify=y_bert\n",
        ")\n",
        "\n",
        "val_size_adjusted = 0.1 / (0.8 + 0.1)\n",
        "X_train_ids, X_val_ids, X_train_masks, X_val_masks, y_train_bert, y_val_bert = train_test_split(\n",
        "    X_temp_ids, X_temp_masks, y_temp_bert, test_size=val_size_adjusted, random_state=42, stratify=y_temp_bert\n",
        ")\n",
        "\n",
        "print(\"Kích thước dữ liệu BERT:\")\n",
        "print(f\"Train: {X_train_ids.shape[0]}\")\n",
        "print(f\"Validation: {X_val_ids.shape[0]}\")\n",
        "print(f\"Test: {X_test_ids.shape[0]}\")"
      ],
      "metadata": {
        "id": "J--8PNbHLuM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Tạo DataLoader\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = FakeNewsDataset(X_train_ids, X_train_masks, y_train_bert)\n",
        "val_dataset = FakeNewsDataset(X_val_ids, X_val_masks, y_val_bert)\n",
        "test_dataset = FakeNewsDataset(X_test_ids, X_test_masks, y_test_bert)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "h36YhsBHLwkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Định nghĩa mô hình BERT\n",
        "class BertForFakeNews(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased', num_classes=2, dropout_rate=0.4):\n",
        "        super(BertForFakeNews, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "T7VTAFoKL2re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Khởi tạo mô hình\n",
        "model = BertForFakeNews()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "_ug3bA0NL6gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Thiết lập optimizer và scheduler\n",
        "epochs = 10\n",
        "learning_rate = 1e-5\n",
        "warmup_steps = 0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "metadata": {
        "id": "E3QspvZZL66j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Hàm train\n",
        "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{correct_predictions/total_predictions:.4f}'\n",
        "        })\n",
        "\n",
        "    return total_loss / len(data_loader), correct_predictions / total_predictions\n"
      ],
      "metadata": {
        "id": "Iu-eEN9PMC_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Hàm đánh giá\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "\n",
        "    return avg_loss, accuracy, all_predictions, all_labels"
      ],
      "metadata": {
        "id": "Br4mKgIsMFQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Training loop\n",
        "from tqdm.auto import tqdm\n",
        "print(\"Bắt đầu training...\")\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "best_val_accuracy = 0\n",
        "best_model_state = None\n",
        "patience = 2\n",
        "patience_counter = 0\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 50)\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc, _, _ = evaluate_model(model, val_loader, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Lưu mô hình tốt nhất (dựa trên validation accuracy)\n",
        "    if val_acc > best_val_accuracy:\n",
        "        best_val_accuracy = val_acc\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        print(f'✅ New best model saved! Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Early stopping logic (dựa trên validation loss)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        print(f'✅ Validation loss improved: {val_loss:.4f}')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f'⚠️ Validation loss didn\\'t improve. Patience: {patience_counter}/{patience}')\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'🛑 Early stopping triggered! No improvement for {patience} epochs.')\n",
        "            print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "4zos9a96MIcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Load mô hình tốt nhất và đánh giá trên test set\n",
        "if best_model_state:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "print(\"\\nĐánh giá trên test set...\")\n",
        "test_loss, test_acc, test_predictions, test_labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "DRBYxVRpMZe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Báo cáo chi tiết\n",
        "print(\"\\nBáo cáo phân loại:\")\n",
        "print(classification_report(test_labels, test_predictions,\n",
        "                          target_names=['Fake News', 'Real News']))"
      ],
      "metadata": {
        "id": "GG1rAEogMa-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Ma trận nhầm lẫn\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.title('Ma trận nhầm lẫn - BERT Model')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hp-Pxe4MMci-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Vẽ biểu đồ loss và accuracy\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "num_epochs_trained = len(train_losses)\n",
        "epochs_range = range(1, num_epochs_trained + 1) # Use the actual number of epochs\n",
        "\n",
        "# Loss\n",
        "ax1.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
        "ax1.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
        "ax1.set_title('Model Loss')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Accuracy\n",
        "ax2.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
        "ax2.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
        "ax2.set_title('Model Accuracy')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V7jEU6VMMedP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Lưu mô hình\n",
        "torch.save({\n",
        "    'model_state_dict': best_model_state,\n",
        "    'tokenizer': tokenizer,\n",
        "    'max_length': max_length,\n",
        "    'test_accuracy': test_acc\n",
        "}, 'bert_fake_news_model.pth')\n",
        "\n",
        "print(f\"\\nMô hình đã được lưu với accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "EAoMiVhEMgTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Hàm dự đoán cho văn bản mới\n",
        "def predict_news(text, model, tokenizer, device, max_length=128):\n",
        "    model.eval()\n",
        "\n",
        "    # Tiền xử lý văn bản giống như training data\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "        confidence = probabilities.max().cpu().numpy()\n",
        "\n",
        "    label = \"Real News\" if prediction == 1 else \"Fake News\"\n",
        "    return label, confidence\n",
        "\n",
        "# Ví dụ sử dụng\n",
        "sample_text = \"Breaking news: Scientists discover new species in Amazon rainforest\"\n",
        "prediction, confidence = predict_news(sample_text, model, tokenizer, device)\n",
        "print(f\"\\nVí dụ dự đoán:\")\n",
        "print(f\"Text: {sample_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"Confidence: {confidence:.4f}\")"
      ],
      "metadata": {
        "id": "QhrKEHS3MiWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "paP-4pM0G_Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pickle\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"KẾT LUẬN DỰ ÁN PHÁT HIỆN TIN GIẢ\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n### Vấn đề đặt ra:\")\n",
        "print(\"- **Sự lan truyền của tin giả:** Sự bùng nổ của mạng xã hội và internet đã làm gia tăng tốc độ phát tán của các thông tin sai lệch, gây ảnh hưởng nghiêm trọng đến nhận thức cộng đồng, chính trị, y tế và an ninh xã hội.\")\n",
        "print(\"Nên ta ứng dụng NLP - nhánh nhỏ của Machine learning để dự đoán fake news\")\n",
        "\n",
        "\n",
        "print(\"\\n### Tóm tắt kết quả:\")\n",
        "print(\" - Đầu tiên ta import các thư viện và dataset để training (DataSet_Misinfo_TRUE.csv và DataSet_Misinfo_FAKE.csv) \")\n",
        "print(\" - Tiền xử lý dữ liệu (làm sạch, token hóa, vector hóa), phân tích dữ liệu (EDA) và xây dựng mô hình.\")\n",
        "print(\"- Nhóm em đã thử nghiệm bốn mô hình: Logistic Regression, Naive Bayes (sử dụng TF-IDF), CNN và LSTM và BERT.\")\n",
        "print(\"Kết quả đánh giá trên tập kiểm tra cho thấy:\")\n",
        "print(\"- Logistic Regression: Accuracy = 93.5064935064935 \")\n",
        "print(\"- Naive Bayes: Accuracy = 85.00127323656736 \")\n",
        "print(\"- CNN + LSTM: Test Accuracy = 0.9481, Test Precision = 0.9303, Test Recall = 0.9548}, Test F1-Score = 0.9424\")\n",
        "print(\"- Bert Model: Test Accuracy = 0.9500, Test Precision = 0.9300, Test Recall = 0.9600, Test F1-Score = 0.9444\")\n",
        "\n",
        "\n",
        "print(\"\\n### Điểm mạnh của mô hình:\")\n",
        "print(\"- tiền xử lý ngôn ngữ tự nhiên (NLP) với nltk (stopwords, tokenization, lemmatization) và re (regex) để làm sạch dữ liệu văn bản, giúp cải thiện chất lượng dữ liệu đầu vào.\")\n",
        "print(\"- TF-IDF giúp mô hình hiểu rõ sự quan trọng tương đối của các từ trong từng văn bản và toàn bộ tập dữ liệu.\")\n",
        "print(\"- mô hình CNN giúp phát hiện các đặc trưng cục bộ như cụm từ quan trọng và mô hình LSTM giúp ghi nhớ ngữ cảnh dài hạn. \")\n",
        "print(\"- sử dụng 'EarlyStopping' giúp dừng huấn luyện sớm khi mô hình không còn cải thiện, và `ReduceLROnPlateau` tự động điều chỉnh tốc độ học, giúp mô hình tối ưu nhanh hơn và tránh overfitting.\")\n",
        "\n",
        "print(\"\\n### Điểm yếu của mô hình:\")\n",
        "print(\"- Khó xử lý ngôn ngữ mơ hồ, châm biếm trong tin giả.\")\n",
        "print(\"- Mô hình huấn luyện trên dữ liệu tĩnh, khó thích ứng với sự thay đổi liên tục của tin giả.\")\n",
        "print(\"- TF-IDF thiếu khả năng hiểu ngữ cảnh và ngữ nghĩa sâu.\")\n",
        "print(\"- CNN+LSTM vẫn kém hiệu quả hơn các mô hình Transformer như BERT.\")\n",
        "print(\"- Khả năng giải thích thấp, khó hiểu lý do phân loại của mô hình.\")\n",
        "\n",
        "\n",
        "print(\"\\n### Cải tiến:\")\n",
        "print(\"- Bổ sung đặc trưng nâng cao: nguồn tin, cảm xúc, độ phức tạp ngữ pháp.\")\n",
        "print(\"- Khai thác dữ liệu đa phương thức nếu có hình ảnh, video đi kèm.\")\n",
        "print(\"- Cải thiện xử lý dữ liệu lệch lớp bằng kỹ thuật resampling tốt hơn.\")\n",
        "print(\"- Phân tích lỗi để hiểu và xử lý các trường hợp mô hình dự đoán sai.\")\n",
        "print(\"- Thiết lập quy trình cập nhật mô hình định kỳ theo dữ liệu mới.\")\n",
        "print(\"- Dùng mô hình tổ hợp (ensemble) để tăng độ ổn định và chính xác.\")\n",
        "\n",
        "print(\"\\n### Kết luận:\")\n",
        "print(\"Dự án đã chứng minh hiệu quả của các kỹ thuật NLP và học sâu trong việc phát hiện tin giả.\")\n",
        "print(\"Trong số các mô hình được triển khai, BERT cho kết quả vượt trội nhất với độ chính xác và F1-score cao, cho thấy ưu thế rõ rệt so với các mô hình truyền thống (Logistic Regression, Naive Bayes) và mô hình Deep Learning (CNN + LSTM).\")\n",
        "print(\"Việc ứng dụng mô hình Transformer như BERT giúp mô hình hiểu ngữ cảnh sâu sắc hơn, khắc phục các hạn chế về ngôn ngữ mơ hồ, châm biếm mà các mô hình khác còn gặp khó.\")\n",
        "print(\"Tuy nhiên, để nâng cao hơn nữa hiệu quả, nhóm đề xuất tích hợp thêm các đặc trưng ngữ nghĩa, dữ liệu đa phương thức và quy trình cập nhật mô hình định kỳ.\")\n",
        "print(\"Dự án là bước khởi đầu tiềm năng trong việc ứng dụng AI vào kiểm chứng thông tin, góp phần giảm thiểu tác động tiêu cực của tin giả trong xã hội.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HEWER7tnEuFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}