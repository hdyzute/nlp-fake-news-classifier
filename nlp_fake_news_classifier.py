# -*- coding: utf-8 -*-
"""nlp-fake-news-classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qTtzxntDSqb6jO3EjMQNe_0xYIwWSgti

# **Project**

##**Problem stament :**     

The widespread dissemination of fake news and propaganda presents serious societal risks, including the erosion of public trust, political polarization, manipulation of elections, and the spread of harmful misinformation during crises such as pandemics or conflicts. From an NLP perspective, detecting fake news is fraught with challenges. Linguistically, fake news often mimics the tone and structure of legitimate journalism, making it difficult to distinguish using surface-level features. The absence of reliable and up-to-date labeled datasets, especially across multiple languages and regions, hampers the effectiveness of supervised learning models. Additionally, the dynamic and adversarial nature of misinformation means that malicious actors constantly evolve their language and strategies to bypass detection systems. Cultural context, sarcasm, satire, and implicit bias further complicate automated analysis. Moreover, NLP models risk amplifying biases present in training data, leading to unfair classifications and potential censorship of legitimate content. These challenges underscore the need for cautious, context-aware approaches, as the failure to address them can inadvertently contribute to misinformation, rather than mitigate it.



Use datasets in link : https://drive.google.com/drive/folders/1mrX3vPKhEzxG96OCPpCeh9F8m_QKCM4z?usp=sharing
to complete requirement.

## **About dataset:**

* **True Articles**:

  * **File**: `MisinfoSuperset_TRUE.csv`
  * **Sources**:

    * Reputable media outlets like **Reuters**, **The New York Times**, **The Washington Post**, etc.

* **Fake/Misinformation/Propaganda Articles**:

  * **File**: `MisinfoSuperset_FAKE.csv`
  * **Sources**:

    * **American right-wing extremist websites** (e.g., Redflag Newsdesk, Breitbart, Truth Broadcast Network)
    * **Public dataset** from:

      * Ahmed, H., Traore, I., & Saad, S. (2017): "Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques" *(Springer LNCS 10618)*

#Library
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout,
    Input, Concatenate, GlobalMaxPooling1D, BatchNormalization
)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

from torch.optim import AdamW
from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

"""#Load Data"""

from google.colab import drive
drive.mount('/content/drive')

true_data = pd.read_csv("/content/drive/MyDrive/DataSet_Misinfo_TRUE.csv")
fake_data = pd.read_csv("/content/drive/MyDrive/DataSet_Misinfo_FAKE.csv")

"""#Data Preprocessing"""

true_data['label'] = 1
fake_data['label'] = 0

data = pd.concat([true_data, fake_data], ignore_index=True)

print(data.isnull().sum())

data = data[data['text'].notna() & (data['text'].str.strip() != '')]

# L√†m s·∫°ch vƒÉn b·∫£n
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
cleaned_texts = []

for text in data['text']:
    if not isinstance(text, str):
        cleaned_texts.append('')
        continue
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    cleaned_texts.append(' '.join(tokens))

data['cleaned_text'] = cleaned_texts
data = data[data['cleaned_text'] != '']

# 3. Ki·ªÉm tra m·∫•t c√¢n b·∫±ng l·ªõp
class_counts = data['label'].value_counts()
print("Ph√¢n b·ªë l·ªõp:")
print(class_counts)

plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=data)
plt.title('Ph√¢n b·ªë l·ªõp b√†i b√°o (1: Th·∫≠t, 0: Gi·∫£)')
plt.xlabel('Nh√£n')
plt.ylabel('S·ªë l∆∞·ª£ng')
plt.show()

# 4. Vector h√≥a v·ªõi TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer.fit_transform(data['cleaned_text'])
y = data['label'].values

# 5. Chia d·ªØ li·ªáu (cho TF-IDF)
X_temp, X_test, y_temp, y_test = train_test_split(
    X_tfidf, y, test_size=0.1, random_state=42, stratify=y
)
val_size_adjusted = 0.1 / (0.8 + 0.1)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp
)

# In k√≠ch th∆∞·ªõc c√°c t·∫≠p
print("K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán:", X_train.shape)
print("K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm ƒë·ªãnh:", X_val.shape)
print("K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra:", X_test.shape)

#X·ª≠ l√Ω imbalance
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# 5. Token h√≥a cho BERT
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
max_length = 128
input_ids = []
attention_masks = []

for text in data['cleaned_text']:
    encoding = tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='np'
    )
    input_ids.append(encoding['input_ids'][0])
    attention_masks.append(encoding['attention_mask'][0])

input_ids = np.array(input_ids)
attention_masks = np.array(attention_masks)
y_bert = data['label'].values

"""#Exploratory Data Analysis"""

data.head()

df = data.copy()

print("\n=== LABEL DISTRIBUTION ===")
label_counts = df['label'].value_counts()
print(label_counts)
print(f"Label balance ratio: {label_counts.min()/label_counts.max():.2f}")

plt.figure(figsize=(8, 6))
data['label'].value_counts().plot(kind='bar')
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

print("\n=== TEXT LENGTH ANALYSIS ===")
df['text_length'] = df['text'].str.len()
df['cleaned_text_length'] = df['cleaned_text'].str.len()
df['word_count'] = df['text'].str.split().str.len()
df['cleaned_word_count'] = df['cleaned_text'].str.split().str.len()

print(f"Original text length - Mean: {df['text_length'].mean():.2f}, Std: {df['text_length'].std():.2f}")
print(f"Cleaned text length - Mean: {df['cleaned_text_length'].mean():.2f}, Std: {df['cleaned_text_length'].std():.2f}")
print(f"Word count - Mean: {df['word_count'].mean():.2f}, Std: {df['word_count'].std():.2f}")

# Plot text length distributions
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Text length histogram
axes[0,0].hist(df['text_length'], bins=50, alpha=0.7, color='blue')
axes[0,0].set_title('Original Text Length Distribution')
axes[0,0].set_xlabel('Character Count')
axes[0,0].set_ylabel('Frequency')

# Cleaned text length histogram
axes[0,1].hist(df['cleaned_text_length'], bins=50, alpha=0.7, color='green')
axes[0,1].set_title('Cleaned Text Length Distribution')
axes[0,1].set_xlabel('Character Count')
axes[0,1].set_ylabel('Frequency')

# Word count histogram
axes[1,0].hist(df['word_count'], bins=50, alpha=0.7, color='red')
axes[1,0].set_title('Word Count Distribution')
axes[1,0].set_xlabel('Word Count')
axes[1,0].set_ylabel('Frequency')

# Text length by label
df.boxplot(column='text_length', by='label', ax=axes[1,1])
axes[1,1].set_title('Text Length by Label')
axes[1,1].set_xlabel('Label')
axes[1,1].set_ylabel('Character Count')

plt.tight_layout()
plt.show()

# 4. VOCABULARY
from collections import Counter
print("\n=== VOCABULARY ANALYSIS ===")

# Function to get word frequencies
def get_word_freq(text_series, n_top=20):
    all_words = ' '.join(text_series.astype(str)).lower().split()
    # Remove basic punctuation
    all_words = [re.sub(r'[^a-zA-Z]', '', word) for word in all_words]
    all_words = [word for word in all_words if len(word) > 0]
    return Counter(all_words).most_common(n_top)

# Overall word frequencies
top_words_original = get_word_freq(df['text'])
top_words_cleaned = get_word_freq(df['cleaned_text'])

print("Top 20 words in original text:")
for word, freq in top_words_original:
    print(f"{word}: {freq}")

print("\nTop 20 words in cleaned text:")
for word, freq in top_words_cleaned:
    print(f"{word}: {freq}")

# Plot top words
fig, axes = plt.subplots(1, 2, figsize=(20, 8))

# Original text top words
words_orig, freqs_orig = zip(*top_words_original)
axes[0].barh(words_orig, freqs_orig)
axes[0].set_title('Top 20 Words in Original Text')
axes[0].set_xlabel('Frequency')

# Cleaned text top words
words_clean, freqs_clean = zip(*top_words_cleaned)
axes[1].barh(words_clean, freqs_clean)
axes[1].set_title('Top 20 Words in Cleaned Text')
axes[1].set_xlabel('Frequency')

plt.tight_layout()
plt.show()

# 5. WORD FREQUENCY BY LABEL
print("\n=== WORD FREQUENCY BY LABEL ===")
for label in df['label'].unique():
    print(f"\nTop words for label {label}:")
    label_text = df[df['label'] == label]['cleaned_text']
    top_words_label = get_word_freq(label_text, 15)
    for word, freq in top_words_label:
        print(f"{word}: {freq}")

# 9. TEXT COMPARISON (Original vs Cleaned)
print("\n=== TEXT COMPARISON ===")
df['compression_ratio'] = df['cleaned_text_length'] / df['text_length']
print(f"Average compression ratio: {df['compression_ratio'].mean():.2f}")
print(f"Text reduction: {(1 - df['compression_ratio'].mean()) * 100:.1f}%")

print("\n=== SUMMARY STATISTICS ===")
summary_stats = {
    'Total Samples': len(df),
    'Unique Texts': df['text'].nunique(),
    'Average Text Length': df['text_length'].mean(),
    'Average Word Count': df['word_count'].mean(),
    'Labels': df['label'].nunique(),
    'Class Balance': df['label'].value_counts().min() / df['label'].value_counts().max()
}

for key, value in summary_stats.items():
    if isinstance(value, float):
        print(f"{key}: {value:.2f}")
    else:
        print(f"{key}: {value}")

"""#Model Building

Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train_resampled, y_train_resampled)
Accuracy = logreg.score(X_test, y_test)
print(Accuracy*100)

X = "The French government announced a national day of mourning after the attack, and President Emmanuel Macron expressed solidarity with the victims during a press conference in Paris."
encoding = tokenizer(
    X,
    padding='max_length',
    truncation=True,
    max_length=max_length,
    return_tensors='np'
)


X_transformed = vectorizer.transform([X])

prediction_proba_logreg = logreg.predict_proba(X_transformed)

probability_true_logreg = prediction_proba_logreg[0][1]

if probability_true_logreg >= 0.5:
    print("This news is True")
else:
    print("This news is False")

X = "Aliens have landed in Germany and established a new world order, claims anonymous source from social media."
encoding = tokenizer(
    X,
    padding='max_length',
    truncation=True,
    max_length=max_length,
    return_tensors='np'
)


X_transformed = vectorizer.transform([X])

prediction_proba_logreg = logreg.predict_proba(X_transformed)

probability_true_logreg = prediction_proba_logreg[0][1]

if probability_true_logreg >= 0.5:
    print("This news is True")
else:
    print("This news is False")

"""Naive-Bayes"""

from sklearn.naive_bayes import MultinomialNB
NB = MultinomialNB()
NB.fit(X_train_resampled, y_train_resampled)
Accuracy = NB.score(X_test, y_test)
print(Accuracy*100)

X = "The French government announced a national day of mourning after the attack, and President Emmanuel Macron expressed solidarity with the victims during a press conference in Paris."
encoding = tokenizer(
    X,
    padding='max_length',
    truncation=True,
    max_length=max_length,
    return_tensors='np'
)


X_transformed = vectorizer.transform([X])

prediction_proba_NB = NB.predict_proba(X_transformed)

probability_true_NB = prediction_proba_NB[0][1]

if probability_true_NB >= 0.5:
    print("This news is True")
else:
    print("This news is False")

X = "Aliens have landed in Germany and established a new world order, claims anonymous source from social media."
encoding = tokenizer(
    X,
    padding='max_length',
    truncation=True,
    max_length=max_length,
    return_tensors='np'
)


X_transformed = vectorizer.transform([X])

prediction_proba_NB = NB.predict_proba(X_transformed)

probability_true_NB = prediction_proba_NB[0][1]

if probability_true_NB >= 0.5:
    print("This news is True")
else:
    print("This news is False")

"""#CNN + LSTM (Keras)

##Hyperparameter set up
"""

# C·∫•u h√¨nh tham s·ªë
MAX_VOCAB_SIZE = 20000
MAX_SEQUENCE_LENGTH = 200
EMBEDDING_DIM = 100
BATCH_SIZE = 32
EPOCHS = 10

"""##Data Preprocessing"""

# Tokenization cho CNN + LSTM
print("ƒêang th·ª±c hi·ªán tokenization...")
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(data['cleaned_text'])

# Chuy·ªÉn ƒë·ªïi text th√†nh sequences
sequences = tokenizer.texts_to_sequences(data['cleaned_text'])
X_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')
y_sequences = data['label'].values

print(f"Vocab size: {len(tokenizer.word_index)}")
print(f"Shape c·ªßa X_sequences: {X_sequences.shape}")

# Chia d·ªØ li·ªáu cho CNN + LSTM
X_temp_seq, X_test_seq, y_temp_seq, y_test_seq = train_test_split(
    X_sequences, y_sequences, test_size=0.1, random_state=42, stratify=y_sequences
)
val_size_adjusted = 0.1 / (0.8 + 0.1)
X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(
    X_temp_seq, y_temp_seq, test_size=val_size_adjusted, random_state=42, stratify=y_temp_seq
)

print("K√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu cho CNN + LSTM:")
print(f"Train: {X_train_seq.shape}, Val: {X_val_seq.shape}, Test: {X_test_seq.shape}")

# X·ª≠ l√Ω class imbalance v·ªõi class weights
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train_seq),
    y=y_train_seq
)
class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}
print(f"Class weights: {class_weight_dict}")

"""##Model Training"""

# ƒê·ªãnh nghƒ©a m√¥ h√¨nh CNN + LSTM
def create_cnn_lstm_model(vocab_size, embedding_dim, max_length):
    """
    T·∫°o m√¥ h√¨nh CNN + LSTM cho ph√¢n lo·∫°i fake news
    """
    model = Sequential()

    # Embedding layer
    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, name='embedding'))

    # CNN layers
    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', name='conv1d_1'))
    model.add(MaxPooling1D(pool_size=2, name='maxpool1d_1'))
    model.add(Dropout(0.3, name='dropout_1'))

    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', name='conv1d_2'))
    model.add(MaxPooling1D(pool_size=2, name='maxpool1d_2'))
    model.add(Dropout(0.3, name='dropout_2'))

    # LSTM layers
    model.add(LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3, name='lstm_1'))
    model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.3, name='lstm_2'))

    # Dense layers
    model.add(Dense(64, activation='relu', name='dense_1'))
    model.add(BatchNormalization(name='batch_norm'))
    model.add(Dropout(0.5, name='dropout_3'))
    model.add(Dense(32, activation='relu', name='dense_2'))
    model.add(Dropout(0.3, name='dropout_4'))

    # Output layer
    model.add(Dense(1, activation='sigmoid', name='output'))

    return model

# T·∫°o v√† compile model
print("ƒêang t·∫°o m√¥ h√¨nh CNN + LSTM...")
vocab_size = min(MAX_VOCAB_SIZE, len(tokenizer.word_index) + 1)
print(f"Vocab size ƒë∆∞·ª£c s·ª≠ d·ª•ng: {vocab_size}")

model = create_cnn_lstm_model(
    vocab_size=vocab_size,
    embedding_dim=EMBEDDING_DIM,
    max_length=MAX_SEQUENCE_LENGTH
)

# Build model b·∫±ng c√°ch truy·ªÅn input shape
model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))

# Compile model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'precision', 'recall']
)

# In th√¥ng tin m√¥ h√¨nh
print("\nKi·∫øn tr√∫c m√¥ h√¨nh:")
model.summary()

# Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

# Training
print("\nB·∫Øt ƒë·∫ßu training...")
history = model.fit(
    X_train_seq, y_train_seq,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(X_val_seq, y_val_seq),
    class_weight=class_weight_dict,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# ƒê√°nh gi√° m√¥ h√¨nh
print("\n" + "="*50)
print("ƒê√ÅNH GI√Å M√î H√åNH CNN + LSTM")
print("="*50)

# D·ª± ƒëo√°n tr√™n t·∫≠p test
test_predictions = model.predict(X_test_seq)
test_pred_binary = (test_predictions > 0.5).astype(int).flatten()

"""##Performance Evaluation"""

# T√≠nh to√°n metrics
test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test_seq, y_test_seq, verbose=0)
f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Precision: {test_precision:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Test F1-Score: {f1_score:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test_seq, test_pred_binary, target_names=['Fake', 'Real']))

# Confusion Matrix
cm = confusion_matrix(y_test_seq, test_pred_binary)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.title('Confusion Matrix - CNN + LSTM')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# V·∫Ω bi·ªÉu ƒë·ªì training history
def plot_training_history(history):
    """V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh training"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')
    axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # Loss
    axes[0, 1].plot(history.history['loss'], label='Train Loss')
    axes[0, 1].plot(history.history['val_loss'], label='Val Loss')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # Precision
    axes[1, 0].plot(history.history['precision'], label='Train Precision')
    axes[1, 0].plot(history.history['val_precision'], label='Val Precision')
    axes[1, 0].set_title('Model Precision')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # Recall
    axes[1, 1].plot(history.history['recall'], label='Train Recall')
    axes[1, 1].plot(history.history['val_recall'], label='Val Recall')
    axes[1, 1].set_title('Model Recall')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Recall')
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

def predict_news(text, model, tokenizer, max_length=MAX_SEQUENCE_LENGTH):
    """
    D·ª± ƒëo√°n m·ªôt b√†i b√°o l√† th·∫≠t hay gi·∫£
    """
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    cleaned_text = ' '.join(tokens)

    # Tokenize v√† padding
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')

    # D·ª± ƒëo√°n
    prediction = model.predict(padded_sequence)[0][0]

    return {
        'prediction': prediction,
        'label': 'Real' if prediction > 0.5 else 'Fake',
        'confidence': prediction if prediction > 0.5 else 1 - prediction
    }

sample_text = "Breaking news: Scientists discover new planet in our solar system"
result = predict_news(sample_text, model, tokenizer)
print(f"\nV√≠ d·ª• d·ª± ƒëo√°n:")
print(f"Text: {sample_text}")
print(f"Prediction: {result['label']} (confidence: {result['confidence']:.4f})")

# L∆∞u model
model.save('cnn_lstm_fake_news_model.h5')
print("\nM√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'cnn_lstm_fake_news_model.h5'")

import pickle
with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)
print("Tokenizer ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'tokenizer.pkl'")

print("\n" + "="*50)

"""#BERT"""

# Thi·∫øt l·∫≠p device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# 1. T·∫°o Dataset class cho BERT
class FakeNewsDataset(Dataset):
    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),
            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

# 2. Chia d·ªØ li·ªáu cho BERT
from sklearn.model_selection import train_test_split

# Chia d·ªØ li·ªáu v·ªõi t·ª∑ l·ªá 80:10:10
X_temp_ids, X_test_ids, X_temp_masks, X_test_masks, y_temp_bert, y_test_bert = train_test_split(
    input_ids, attention_masks, y_bert, test_size=0.1, random_state=42, stratify=y_bert
)

val_size_adjusted = 0.1 / (0.8 + 0.1)
X_train_ids, X_val_ids, X_train_masks, X_val_masks, y_train_bert, y_val_bert = train_test_split(
    X_temp_ids, X_temp_masks, y_temp_bert, test_size=val_size_adjusted, random_state=42, stratify=y_temp_bert
)

print("K√≠ch th∆∞·ªõc d·ªØ li·ªáu BERT:")
print(f"Train: {X_train_ids.shape[0]}")
print(f"Validation: {X_val_ids.shape[0]}")
print(f"Test: {X_test_ids.shape[0]}")

# 3. T·∫°o DataLoader
batch_size = 16

train_dataset = FakeNewsDataset(X_train_ids, X_train_masks, y_train_bert)
val_dataset = FakeNewsDataset(X_val_ids, X_val_masks, y_val_bert)
test_dataset = FakeNewsDataset(X_test_ids, X_test_masks, y_test_bert)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 4. ƒê·ªãnh nghƒ©a m√¥ h√¨nh BERT
class BertForFakeNews(nn.Module):
    def __init__(self, model_name='bert-base-uncased', num_classes=2, dropout_rate=0.4):
        super(BertForFakeNews, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

# 5. Kh·ªüi t·∫°o m√¥ h√¨nh
model = BertForFakeNews()
model.to(device)

# 6. Thi·∫øt l·∫≠p optimizer v√† scheduler
epochs = 10
learning_rate = 1e-5
warmup_steps = 0

optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)
total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# 7. H√†m train
def train_epoch(model, data_loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(data_loader, desc="Training")

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = nn.CrossEntropyLoss()(logits, labels)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

        predictions = torch.argmax(logits, dim=1)
        correct_predictions += (predictions == labels).sum().item()
        total_predictions += labels.size(0)

        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{correct_predictions/total_predictions:.4f}'
        })

    return total_loss / len(data_loader), correct_predictions / total_predictions

# 8. H√†m ƒë√°nh gi√°
def evaluate_model(model, data_loader, device):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0
    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = nn.CrossEntropyLoss()(logits, labels)

            total_loss += loss.item()

            predictions = torch.argmax(logits, dim=1)
            correct_predictions += (predictions == labels).sum().item()
            total_predictions += labels.size(0)

            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = correct_predictions / total_predictions
    avg_loss = total_loss / len(data_loader)

    return avg_loss, accuracy, all_predictions, all_labels

# 9. Training loop
from tqdm.auto import tqdm
print("B·∫Øt ƒë·∫ßu training...")
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# Early stopping parameters
best_val_loss = float('inf')
best_val_accuracy = 0
best_model_state = None
patience = 2
patience_counter = 0


for epoch in range(epochs):
    print(f'\nEpoch {epoch + 1}/{epochs}')
    print('-' * 50)

    # Training
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)

    # Validation
    val_loss, val_acc, _, _ = evaluate_model(model, val_loader, device)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)

    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')
    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

    # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t (d·ª±a tr√™n validation accuracy)
    if val_acc > best_val_accuracy:
        best_val_accuracy = val_acc
        best_model_state = model.state_dict().copy()
        print(f'‚úÖ New best model saved! Val Acc: {val_acc:.4f}')

    # Early stopping logic (d·ª±a tr√™n validation loss)
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        print(f'‚úÖ Validation loss improved: {val_loss:.4f}')
    else:
        patience_counter += 1
        print(f'‚ö†Ô∏è Validation loss didn\'t improve. Patience: {patience_counter}/{patience}')

        if patience_counter >= patience:
            print(f'üõë Early stopping triggered! No improvement for {patience} epochs.')
            print(f'Best validation accuracy: {best_val_accuracy:.4f}')
            break

# 10. Load m√¥ h√¨nh t·ªët nh·∫•t v√† ƒë√°nh gi√° tr√™n test set
if best_model_state:
    model.load_state_dict(best_model_state)

print("\nƒê√°nh gi√° tr√™n test set...")
test_loss, test_acc, test_predictions, test_labels = evaluate_model(model, test_loader, device)

print(f'Test Loss: {test_loss:.4f}')
print(f'Test Accuracy: {test_acc:.4f}')

# 11. B√°o c√°o chi ti·∫øt
print("\nB√°o c√°o ph√¢n lo·∫°i:")
print(classification_report(test_labels, test_predictions,
                          target_names=['Fake News', 'Real News']))

# 12. Ma tr·∫≠n nh·∫ßm l·∫´n
plt.figure(figsize=(8, 6))
cm = confusion_matrix(test_labels, test_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n - BERT Model')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# 13. V·∫Ω bi·ªÉu ƒë·ªì loss v√† accuracy
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

num_epochs_trained = len(train_losses)
epochs_range = range(1, num_epochs_trained + 1) # Use the actual number of epochs

# Loss
ax1.plot(epochs_range, train_losses, 'bo-', label='Training Loss')
ax1.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')
ax1.set_title('Model Loss')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True)

# Accuracy
ax2.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')
ax2.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')
ax2.set_title('Model Accuracy')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Accuracy')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

# 14. L∆∞u m√¥ h√¨nh
torch.save({
    'model_state_dict': best_model_state,
    'tokenizer': tokenizer,
    'max_length': max_length,
    'test_accuracy': test_acc
}, 'bert_fake_news_model.pth')

print(f"\nM√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v·ªõi accuracy: {test_acc:.4f}")

# Save the BERT tokenizer
from transformers import AutoTokenizer

tokenizer_dir = 'bert_tokenizer'
# Load the BERT tokenizer explicitly
bert_tokenizer_to_save = AutoTokenizer.from_pretrained('bert-base-uncased')

# Save the BERT tokenizer
bert_tokenizer_to_save.save_pretrained(tokenizer_dir)

print(f"BERT tokenizer saved to directory: {tokenizer_dir}")